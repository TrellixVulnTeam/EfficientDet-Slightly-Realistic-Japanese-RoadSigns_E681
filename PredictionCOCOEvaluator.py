# Copyright 2022 antillia.com Toshiyuki Arai 
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# 2022/06/04 copyright (c) antillia.com
# PredictionCOCOEvaluator.py
# Compute a map of prediction of object detection to a test_dataset 
# from 
# coco_prdiction_to_test_dataset.jscon and
# coco_ground_truth_to_test_dataset.json   

import os
import sys
import traceback
import glob
import json
import tempfile
import io
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

# See also: https://jpcodeqa.com/q/31e93ca94758228f859e01a8ee315b15

class PredictionCOCOEvaluator:

  def __init__(self, image_path_pattern):
    self.image_list = glob.glob(image_path_pattern)

    if self.image_list == None or self.image_list==[]:
      raise Exception("No image files in " + images_dir)

  def run(self, all_predictions_dir, annotations_file, output_dir):
    print("=== PredictionEvaluator.run all_predictions {} annotation_file {}".format(all_predictions_dir, annotations_file))
    # coco_prdiction_to_test_dataset.jscon
    prediction = all_predictions_dir + "/all_prediction.json"
           
    output_filepath = os.path.join(output_dir, "prediction_map.csv")
  
    SEP = ","
    NL  = "\n"
    HEADER = "prediction, map" + NL

    with open(output_filepath, "w") as f:
      f.writelines(HEADER)
      evaluation = {}
          
      with open(prediction, mode='rt', encoding='utf-8') as file:
        data = json.load(file)   
        pred = data['predictions']     
        map, detail = self.summarize(pred, annotations_file, self.image_list)
        map   = round(map, 3)
        print("=== prediction {} map {}".format(prediction, map))
        #print(detail)
        pred_base = os.path.basename(prediction)
        evaluation[str(pred_base)] = map

      print("--- evaluation {}".format(evaluation))
      evaluation_sorted = sorted(evaluation.items(), key=lambda x:x[0])
      print("--- evaluation_sorted {}".format(evaluation_sorted))

      for item in evaluation_sorted:
        (name, value) = item
        line = str(name) + SEP + str(value) + NL
        f.writelines(line)

  # https://www.programcreek.com/python/example/88588/pycocotools.cocoeval.COCOeval
  # Example 1
  """
  def evaluate():
    cocoGt = COCO('annotations.json')
    cocoDt = cocoGt.loadRes('detections.json')
    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize()
  """
  def summarize(self, predictions, annotations_file, iou_type="bbox"):
    #print("=== PredictionEvaluator.summarize {}".format( predictions))
    #print("=== annotations file " + annotations_file)
    # predictions:     all_prediction.json generated by Inference Engine.
    # annotation_file: Ground Truth 
    cocoGt = COCO(annotations_file)
    # Prediction/Detection
    #print("---------------------predictions {}".format(predictions))
    cocoDt = cocoGt.loadRes(predictions)

    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')
    cocoEval.evaluate()
    cocoEval.accumulate()

    original_stdout = sys.stdout
    string_stdout = io.StringIO()
    sys.stdout = string_stdout
    cocoEval.summarize()
    sys.stdout = original_stdout

    mean_ap = cocoEval.stats[0].item()  # stats[0] records AP@[0.5:0.95]
    detail  = string_stdout.getvalue()
    ar      = cocoEval.stats[6].item()
    mean_ap = round(mean_ap, 3)
    ar      = round(ar, 3)
    print("--- mean_ap AP@[0.5:0.95] {}".format(mean_ap))
    print("--- mean_ar AP@[0.5:0.95] {}".format(ar))
    print("--- detail")
    print("{}".format(detail))
    #for stats in cocoEval.stats:
    #  print("-- {}".format(stats.item()))
    return mean_ap, detail 
  
"""
Usage:
python ./PredictionCOCOEvaluator.py ./predictions_dir ./ground_truth_annotation.json ./image_path_pattern ./output_dir

python ../../PredictionCOCOEvaluator.py ^
  ./realistic_test_dataset_output/ ^
  ./realistic_test_dataset/annotation.json ^
  ./realistic_test_dataset/*.jpg ^
  ./prediction_eval
"""
#
if __name__ == "__main__":
  predictions_dir    = ""  
  annotations_file   = ""  # ground_truth_json to a test_dataset
  image_path_pattern = ""
  output_dir         = "./evaluated"
  try:
    if len(sys.argv) == 5:
      predictions_dir    = sys.argv[1]  # prediction output dir for the images in a test_dataset folder. 
      annotations_file   = sys.argv[2]
      image_path_pattern = sys.argv[3]
      output_dir         = sys.argv[4]
    else:
      raise Exception("Invalid argment ")

    if os.path.exists(predictions_dir) == False:
      raise Exception("Not found predictions_dir  " + predictions_dir)

    if os.path.exists(annotations_file)== False:
      raise Exception("Not found annotations_file  " + annotations_file)

    #if os.path.exists(images_dir)==False:
    #  raise Exception("Not found images_dir  " + images_dir)

    if os.path.exists(output_dir)==False:
      os.makedirs(output_dir)

    evaluator = PredictionCOCOEvaluator(image_path_pattern)
    evaluator.run(predictions_dir, annotations_file, output_dir)

  except:
    traceback.print_exc()
